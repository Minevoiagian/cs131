{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "82bd2b57",
      "metadata": {
        "id": "82bd2b57",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "filePath = \"student_habits_performance.csv\"\n",
        "#make the spark session and import the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "368ff544",
      "metadata": {
        "id": "368ff544",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"TestApp\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c8129594",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8129594",
        "outputId": "e92f6d8a-7c1d-4a43-fa39-97103ec7de9e",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original top 5 rows\n",
            "+----------+---+------+-------------------+------------------+-------------+-------------+---------------------+-----------+------------+------------------+------------------------+----------------+--------------------+-----------------------------+----------+\n",
            "|student_id|age|gender|study_hours_per_day|social_media_hours|netflix_hours|part_time_job|attendance_percentage|sleep_hours|diet_quality|exercise_frequency|parental_education_level|internet_quality|mental_health_rating|extracurricular_participation|exam_score|\n",
            "+----------+---+------+-------------------+------------------+-------------+-------------+---------------------+-----------+------------+------------------+------------------------+----------------+--------------------+-----------------------------+----------+\n",
            "|     S1000| 23|Female|                0.0|               1.2|          1.1|           No|                 85.0|        8.0|        Fair|                 6|                  Master|         Average|                   8|                          Yes|      56.2|\n",
            "|     S1001| 20|Female|                6.9|               2.8|          2.3|           No|                 97.3|        4.6|        Good|                 6|             High School|         Average|                   8|                           No|     100.0|\n",
            "|     S1002| 21|  Male|                1.4|               3.1|          1.3|           No|                 94.8|        8.0|        Poor|                 1|             High School|            Poor|                   1|                           No|      34.3|\n",
            "|     S1003| 23|Female|                1.0|               3.9|          1.0|           No|                 71.0|        9.2|        Poor|                 4|                  Master|            Good|                   1|                          Yes|      26.8|\n",
            "|     S1004| 19|Female|                5.0|               4.4|          0.5|           No|                 90.9|        4.9|        Fair|                 3|                  Master|            Good|                   1|                           No|      66.4|\n",
            "+----------+---+------+-------------------+------------------+-------------+-------------+---------------------+-----------+------------+------------------+------------------------+----------------+--------------------+-----------------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#read the dataset, and then get the rows, debug as well to make sure things work\n",
        "taxiDF = spark.read.csv(filePath, header=True,inferSchema = True)\n",
        "print(\"Original top 5 rows\")\n",
        "taxiDF.select(\"student_id\",\"age\",\"gender\",\"study_hours_per_day\",\"social_media_hours\",\"netflix_hours\",\"part_time_job\",\"attendance_percentage\",\"sleep_hours\",\"diet_quality\",\"exercise_frequency\",\"parental_education_level\",\"internet_quality\",\"mental_health_rating\",\"extracurricular_participation\",\"exam_score\").show(5)\n",
        "#split the dataset\n",
        "trainDF, testDF = taxiDF.randomSplit([0.8,0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "    # Assuming 'category_indexed' is the output from StringIndexer\n",
        "    encoder = OneHotEncoder(inputCols=[\"gender\",\n",
        "                                       \"social_media_hours\",\n",
        "                                       \"diet_quality\",\n",
        "                                       \"parental_education_level\",\n",
        "                                       \"internet_quality\",\n",
        "                                       \"extracurricular_participation\"],\n",
        "                             outputCols=[\"gender_encoded\",\"social_media_hours_encoded\", \"diet_quality_encoded\", \"parental_education_level_encoded\", \"internet_quality_encoded\", \"extracurricular_participation_encoded\"])\n",
        "\n",
        "    encoded_df = encoder.fit(taxiDF).transform(taxiDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "KCksn3hYpLjv",
        "outputId": "4eaf9119-3012-453b-ed27-369a6d7199db"
      },
      "id": "KCksn3hYpLjv",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Column gender must be of type numeric but was actually of type string.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-2484713116.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                          outputCols=[\"gender_encoded\",\"social_media_hours_encoded\", \"diet_quality_encoded\", \"parental_education_level_encoded\", \"internet_quality_encoded\", \"extracurricular_participation_encoded\"])\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxiDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxiDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column gender must be of type numeric but was actually of type string."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffa6199",
      "metadata": {
        "id": "9ffa6199",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#transformer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "vecAssembler = VectorAssembler(inputCols=[\"student_id\",\"age\",\"gender_encoded\",\"study_hours_per_day\",\"social_media_hours_encoded\",\"netflix_hours\",\"part_time_job\",\"attendance_percentage\",\"sleep_hours\",\"diet_quality_encoded\",\"exercise_frequency\",\"parental_education_level_encoded\",\"internet_quality_encoded\",\"mental_health_rating\",\"extracurricular_participation_encoded\"], outputCol=\"features\")\n",
        "\n",
        "vecTrainDF = vecAssembler.transform(trainDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7cf446",
      "metadata": {
        "id": "4c7cf446",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#make the decision tree model\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\",labelCol=\"exam_score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42aa00d1",
      "metadata": {
        "id": "42aa00d1",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#train the model\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline (stages=[vecAssembler,dt])\n",
        "pipelineModel = pipeline.fit(trainDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fe2d15",
      "metadata": {
        "id": "63fe2d15",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#test dataset stuff\n",
        "predDF = pipelineModel.transform(testDF)\n",
        "print(\"Prediction 10 rows\")\n",
        "predDF.select(\"student_id\",\"age\",\"gender\",\"study_hours_per_day\",\"social_media_hours\",\"netflix_hours\",\"part_time_job\",\"attendance_percentage\",\"sleep_hours\",\"diet_quality\",\"exercise_frequency\",\"parental_education_level\",\"internet_quality\",\"mental_health_rating\",\"extracurricular_participation\",\"prediction\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b6f1b3",
      "metadata": {
        "id": "11b6f1b3",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#check RMSE\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "regressionEvaluator = RegressionEvaluator(\n",
        "    predictionCol=\"prediction\",\n",
        "    labelCol=\"total_amount\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "rmse = regressionEvaluator.evaluate(predDF)\n",
        "print(\"RMSE: \",rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998fabbe",
      "metadata": {
        "id": "998fabbe",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}